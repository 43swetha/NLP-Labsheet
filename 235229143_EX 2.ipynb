{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME:Swetha.K\n",
    "ROLL NO:235229143"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83c3e2",
   "metadata": {},
   "source": [
    "# EXERCISE-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924063b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Vector: [-0.01717069 -0.01692636 -0.02097744 -0.01248638  0.00637875 -0.0224748\n",
      " -0.01894194  0.00689602 -0.00970271  0.01569376  0.00976377  0.02141749\n",
      " -0.02677986  0.01660375  0.00440961 -0.00627108 -0.0114711  -0.00656082\n",
      " -0.02428151  0.0107822 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "docs = [\"the house had a tiny little mouse\",\n",
    "        \"the cat saw the mouse\",\n",
    "        \"the mouse ran away from the house\",\n",
    "        \"the cat finally ate the mouse\",\n",
    "        \"the end of the mouse story\"]\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in docs]\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(tokenized_docs)]\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "model = Doc2Vec(vector_size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=30)\n",
    "model.save(\"d2v_model\")\n",
    "new_document = \"a tiny mouse\"\n",
    "inferred_vector = model.infer_vector(word_tokenize(new_document.lower()))\n",
    "print(\"Inferred Vector:\", inferred_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8050d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar documents:\n",
      "1. Document 0: Similarity 0.38970947265625\n",
      "2. Document 1: Similarity 0.17346727848052979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1mscds43\\AppData\\Local\\Temp\\ipykernel_10708\\1704727005.py:16: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_documents = model.docvecs.most_similar([inferred_vector], topn=2)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "model = Doc2Vec.load(\"d2v_model\")\n",
    "query_document = \"cat stayed in the house\"\n",
    "tokenized_query = word_tokenize(query_document.lower())\n",
    "tor = model.\n",
    "inferred_vecinfer_vector(tokenized_query)\n",
    "similar_documents = model.docvecs.most_similar([inferred_vector], topn=2)\n",
    "indices, similarities = zip(*similar_documents)\n",
    "print(\"Most similar documents:\")\n",
    "for i, index in enumerate(indices):\n",
    "    print(f\"{i+1}. Document {index}: Similarity {similarities[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb719f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
